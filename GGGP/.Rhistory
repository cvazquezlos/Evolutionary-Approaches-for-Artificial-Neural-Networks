evaluation <- function(individual, mode) {
# Neurons extraction
hidden_layers <- numeric(0)
i <- 0
for (layer in head(strsplit(individual$architecture[[1]], "/")[[1]], -1)) {
if (i != 0) {
hidden_layers[i] <- nchar(layer)
}
i <- i + 1
}
# Individual evaluation
model <- keras_model_sequential()
model %>% layer_dense(units = hidden_layers[1], input_shape = c(I), activation = "relu")
for (layer in tail(hidden_layers, 1)) {
model %>% layer_dense(units = layer, activation = "relu")
}
if (mode == 0) {
model %>% layer_dense(units = O, activation = "softmax")
model %>% compile(
optimizer = "adam",
loss = "categorical_crossentropy",
metrics = c("accuracy")
)
} else {
# TODO: Regression NN output layer.
}
history <- model %>% fit(rbind(X_train, X_validation), rbind(y_train, y_validation), validation_split = 0.235294, epochs = 1000, verbose = 0, callbacks = list(
callback_early_stopping(monitor = "val_loss", min_delta = 0, patience = 100, verbose = 1, mode = "auto")
))
png(filename = paste0("data/history/", individual$id, ".png"))
plot(history)
dev.off()
save_model_hdf5(model, paste0("data/model/", individual$architecture, ".h5"))
score <- model %>% evaluate(X_validation, y_validation)
individual$evaluated <- TRUE
individual$loss <- score['loss'][[1]]
individual$metric <- score['acc'][[1]]
individual$saved_model <- paste0("data/model/", individual$architecture, "_", individual$id, ".h5")
return(individual)
}
# Tournament selection
selection <- function(no_childs) {
no_parents <- no_childs
matting_pool <- NULL
for (n in c(1:(no_childs/2))) {
parents_sample <- population[sample(nrow(population), no_parents),]
ordered_parents_sample <- parents_sample[order]
matting_pool <- rbind(matting_pool, head(parents_sample[order(parents_sample$loss)], 2))
}
return(matting_pool)
}
crossover <- function(i1, i2) {
i1_non_terms <- str_count("/", i1$architecture)
}
# MAIN ALGORITHM
data <- read.csv("../datasets/classification/iris.csv", header = T, sep = ",")
n <- nrow(data)
aux <- dummy.data.frame(data, names = c("class"), sep = "")
rm("data")
data <- aux
rm("aux")
shuffled_df <- as.data.frame(data[sample(n),])
colnames(shuffled_df) <- gsub("[^a-zA-Z]*", "", colnames(shuffled_df))
c <- colnames(shuffled_df)
train <- shuffled_df[1: round(0.7*n),]
validation <- shuffled_df[(round(0.7*n)+1):round(0.9*n),]
test <- shuffled_df[(round(0.9*n)+1):n,]
X_train <- train[,head(colnames(shuffled_df), -3)] %>% as.matrix()
y_train <- train[,tail(colnames(shuffled_df), 3)] %>% as.matrix()
X_validation <- validation[,head(colnames(shuffled_df), -3)] %>% as.matrix()
y_validation <- validation[,tail(colnames(shuffled_df), 3)] %>% as.matrix()
X_test <- test[,head(colnames(shuffled_df), -3)] %>% as.matrix()
y_test <- test[,tail(colnames(shuffled_df), 3)] %>% as.matrix()
I <- length(colnames(X_train))
O <- length(colnames(y_train))
population <- generation(10)
for (individual in 1:nrow(population)) {
population[individual,] = evaluation(population[individual,], 0)
}
install_keras(tensorflow = "gpu")
library("dummies")
library("gramEvol")
library("jsonlite")
library("keras")
library("neuralnet")
library("stringr")
install_keras(tensorflow = "gpu")
# Grammar definition
GRAMMAR <- list(
S = gsrule("<a><h>/<z>"),
a = grule("nnnn"), # Update a with many n as value of I.
z = grule("nnn"),  # Update z with many n as value of O.
h = gsrule("<h><h>", "<h><h>", "/<n>"),
n = gsrule("n<n>", "n")
)
I <- 4
id <- 0
O <- 3
generation <- function(n) {
population <- GrammarRandomExpression(CreateGrammar(GRAMMAR), n)
i <- 1
for (individual in population) {
population[[i]] <- list(id = id, architecture = gsub("\"", "", toString(individual)), evaluated = FALSE,
loss = NULL, metric = NULL, saved_model = NULL) # Cleaning up the expression.
i <- i + 1
id <<- id + 1
}
return(as.data.frame(do.call(rbind, population)))
}
evaluation <- function(individual, mode) {
# Neurons extraction
hidden_layers <- numeric(0)
i <- 0
for (layer in head(strsplit(individual$architecture[[1]], "/")[[1]], -1)) {
if (i != 0) {
hidden_layers[i] <- nchar(layer)
}
i <- i + 1
}
# Individual evaluation
model <- keras_model_sequential()
model %>% layer_dense(units = hidden_layers[1], input_shape = c(I), activation = "relu")
for (layer in tail(hidden_layers, 1)) {
model %>% layer_dense(units = layer, activation = "relu")
}
if (mode == 0) {
model %>% layer_dense(units = O, activation = "softmax")
model %>% compile(
optimizer = "adam",
loss = "categorical_crossentropy",
metrics = c("accuracy")
)
} else {
# TODO: Regression NN output layer.
}
history <- model %>% fit(rbind(X_train, X_validation), rbind(y_train, y_validation), validation_split = 0.235294, epochs = 1000, verbose = 0, callbacks = list(
callback_early_stopping(monitor = "val_loss", min_delta = 0, patience = 100, verbose = 1, mode = "auto")
))
png(filename = paste0("data/history/", individual$id, ".png"))
plot(history)
dev.off()
save_model_hdf5(model, paste0("data/model/", individual$architecture, ".h5"))
score <- model %>% evaluate(X_validation, y_validation)
individual$evaluated <- TRUE
individual$loss <- score['loss'][[1]]
individual$metric <- score['acc'][[1]]
individual$saved_model <- paste0("data/model/", individual$architecture, "_", individual$id, ".h5")
return(individual)
}
# Tournament selection
selection <- function(no_childs) {
no_parents <- no_childs
matting_pool <- NULL
for (n in c(1:(no_childs/2))) {
parents_sample <- population[sample(nrow(population), no_parents),]
ordered_parents_sample <- parents_sample[order]
matting_pool <- rbind(matting_pool, head(parents_sample[order(parents_sample$loss)], 2))
}
return(matting_pool)
}
crossover <- function(i1, i2) {
i1_non_terms <- str_count("/", i1$architecture)
}
# MAIN ALGORITHM
data <- read.csv("../datasets/classification/iris.csv", header = T, sep = ",")
n <- nrow(data)
aux <- dummy.data.frame(data, names = c("class"), sep = "")
rm("data")
data <- aux
rm("aux")
shuffled_df <- as.data.frame(data[sample(n),])
colnames(shuffled_df) <- gsub("[^a-zA-Z]*", "", colnames(shuffled_df))
c <- colnames(shuffled_df)
train <- shuffled_df[1: round(0.7*n),]
validation <- shuffled_df[(round(0.7*n)+1):round(0.9*n),]
test <- shuffled_df[(round(0.9*n)+1):n,]
X_train <- train[,head(colnames(shuffled_df), -3)] %>% as.matrix()
y_train <- train[,tail(colnames(shuffled_df), 3)] %>% as.matrix()
X_validation <- validation[,head(colnames(shuffled_df), -3)] %>% as.matrix()
y_validation <- validation[,tail(colnames(shuffled_df), 3)] %>% as.matrix()
X_test <- test[,head(colnames(shuffled_df), -3)] %>% as.matrix()
y_test <- test[,tail(colnames(shuffled_df), 3)] %>% as.matrix()
I <- length(colnames(X_train))
O <- length(colnames(y_train))
population <- generation(10)
for (individual in 1:nrow(population)) {
population[individual,] = evaluation(population[individual,], 0)
}
population <- generation(50)
for (individual in 1:nrow(population)) {
population[individual,] = evaluation(population[individual,], 0)
}
GRAMMAR <- list(
S = gsrule("<a><h>/<z>"),
a = grule("nnnn"), # Update a with many n as value of I.
z = grule("nnn"),  # Update z with many n as value of O.
h = gsrule("<h><h>", "<h><h>", "/<n>"),
n = gsrule("n<n>", "n")
)
I <- 4
id <- 0
O <- 3
generation <- function(n) {
population <- GrammarRandomExpression(CreateGrammar(GRAMMAR), n)
i <- 1
for (individual in population) {
population[[i]] <- list(id = id, architecture = gsub("\"", "", toString(individual)), evaluated = FALSE,
loss = NULL, metric = NULL, saved_model = NULL) # Cleaning up the expression.
i <- i + 1
id <<- id + 1
}
return(as.data.frame(do.call(rbind, population)))
}
evaluation <- function(individual, mode) {
# Neurons extraction
hidden_layers <- numeric(0)
i <- 0
for (layer in head(strsplit(individual$architecture[[1]], "/")[[1]], -1)) {
if (i != 0) {
hidden_layers[i] <- nchar(layer)
}
i <- i + 1
}
# Individual evaluation
model <- keras_model_sequential()
model %>% layer_dense(units = hidden_layers[1], input_shape = c(I), activation = "relu")
for (layer in tail(hidden_layers, 1)) {
model %>% layer_dense(units = layer, activation = "relu")
}
if (mode == 0) {
model %>% layer_dense(units = O, activation = "softmax")
model %>% compile(
optimizer = "adam",
loss = "categorical_crossentropy",
metrics = c("accuracy")
)
} else {
# TODO: Regression NN output layer.
}
history <- model %>% fit(rbind(X_train, X_validation), rbind(y_train, y_validation), validation_split = 0.235294, epochs = 1000, verbose = 0, callbacks = list(
callback_early_stopping(monitor = "val_loss", min_delta = 0, patience = 100, verbose = 1, mode = "auto")
))
png(filename = paste0("data/history/", individual$id, ".png"))
plot(history)
dev.off()
save_model_hdf5(model, paste0("data/model/", individual$architecture, ".h5"))
score <- model %>% evaluate(X_validation, y_validation)
individual$evaluated <- TRUE
individual$loss <- score['loss'][[1]]
individual$metric <- score['acc'][[1]]
individual$saved_model <- paste0("data/model/", individual$architecture, "/", individual$id, ".h5")
return(individual)
}
# Tournament selection
selection <- function(no_childs) {
no_parents <- no_childs
matting_pool <- NULL
for (n in c(1:(no_childs/2))) {
parents_sample <- population[sample(nrow(population), no_parents),]
ordered_parents_sample <- parents_sample[order]
matting_pool <- rbind(matting_pool, head(parents_sample[order(parents_sample$loss)], 2))
}
return(matting_pool)
}
crossover <- function(i1, i2) {
i1_non_terms <- str_count("/", i1$architecture)
}
# MAIN ALGORITHM
data <- read.csv("../datasets/classification/iris.csv", header = T, sep = ",")
n <- nrow(data)
aux <- dummy.data.frame(data, names = c("class"), sep = "")
rm("data")
data <- aux
rm("aux")
shuffled_df <- as.data.frame(data[sample(n),])
colnames(shuffled_df) <- gsub("[^a-zA-Z]*", "", colnames(shuffled_df))
c <- colnames(shuffled_df)
train <- shuffled_df[1: round(0.7*n),]
validation <- shuffled_df[(round(0.7*n)+1):round(0.9*n),]
test <- shuffled_df[(round(0.9*n)+1):n,]
X_train <- train[,head(colnames(shuffled_df), -3)] %>% as.matrix()
y_train <- train[,tail(colnames(shuffled_df), 3)] %>% as.matrix()
X_validation <- validation[,head(colnames(shuffled_df), -3)] %>% as.matrix()
y_validation <- validation[,tail(colnames(shuffled_df), 3)] %>% as.matrix()
X_test <- test[,head(colnames(shuffled_df), -3)] %>% as.matrix()
y_test <- test[,tail(colnames(shuffled_df), 3)] %>% as.matrix()
I <- length(colnames(X_train))
O <- length(colnames(y_train))
population <- generation(50)
for (individual in 1:nrow(population)) {
population[individual,] = evaluation(population[individual,], 0)
}
str("/", "_", "nnnn/nn/nnn")
str_replace("/", "_", "nnnn/nn/nnn")
str_replace_all("nnn/nn/n", "/", "_")
library("dummies")
library("gramEvol")
library("jsonlite")
library("keras")
library("neuralnet")
library("stringr")
#install_keras(tensorflow = "gpu")
# Grammar definition
GRAMMAR <- list(
S = gsrule("<a><h>/<z>"),
a = grule("nnnn"), # Update a with many n as value of I.
z = grule("nnn"),  # Update z with many n as value of O.
h = gsrule("<h><h>", "<h><h>", "/<n>"),
n = gsrule("n<n>", "n")
)
I <- 4
id <- 0
O <- 3
generation <- function(n) {
population <- GrammarRandomExpression(CreateGrammar(GRAMMAR), n)
i <- 1
for (individual in population) {
population[[i]] <- list(id = id, architecture = gsub("\"", "", toString(individual)), evaluated = FALSE,
loss = NULL, metric = NULL, saved_model = NULL) # Cleaning up the expression.
i <- i + 1
id <<- id + 1
}
return(as.data.frame(do.call(rbind, population)))
}
evaluation <- function(individual, mode) {
# Neurons extraction
hidden_layers <- numeric(0)
i <- 0
for (layer in head(strsplit(individual$architecture[[1]], "/")[[1]], -1)) {
if (i != 0) {
hidden_layers[i] <- nchar(layer)
}
i <- i + 1
}
# Individual evaluation
model <- keras_model_sequential()
model %>% layer_dense(units = hidden_layers[1], input_shape = c(I), activation = "relu")
for (layer in tail(hidden_layers, 1)) {
model %>% layer_dense(units = layer, activation = "relu")
}
if (mode == 0) {
model %>% layer_dense(units = O, activation = "softmax")
model %>% compile(
optimizer = "adam",
loss = "categorical_crossentropy",
metrics = c("accuracy")
)
} else {
# TODO: Regression NN output layer.
}
history <- model %>% fit(rbind(X_train, X_validation), rbind(y_train, y_validation), validation_split = 0.235294, epochs = 1000, verbose = 0, callbacks = list(
callback_early_stopping(monitor = "val_loss", min_delta = 0, patience = 100, verbose = 1, mode = "auto")
))
png(filename = paste0("data/history/", individual$id, ".png"))
plot(history)
dev.off()
save_model_hdf5(model, paste0("data/model/", individual$architecture, ".h5"))
score <- model %>% evaluate(X_validation, y_validation)
individual$evaluated <- TRUE
individual$loss <- score['loss'][[1]]
individual$metric <- score['acc'][[1]]
individual$saved_model <- paste0("data/model/", str_replace_all(individual$architecture, "/", "_"), "_", individual$id, ".h5")
return(individual)
}
# Tournament selection
selection <- function(no_childs) {
no_parents <- no_childs
matting_pool <- NULL
for (n in c(1:(no_childs/2))) {
parents_sample <- population[sample(nrow(population), no_parents),]
ordered_parents_sample <- parents_sample[order]
matting_pool <- rbind(matting_pool, head(parents_sample[order(parents_sample$loss)], 2))
}
return(matting_pool)
}
crossover <- function(i1, i2) {
i1_non_terms <- str_count("/", i1$architecture)
}
# MAIN ALGORITHM
data <- read.csv("../datasets/classification/iris.csv", header = T, sep = ",")
n <- nrow(data)
aux <- dummy.data.frame(data, names = c("class"), sep = "")
rm("data")
data <- aux
rm("aux")
shuffled_df <- as.data.frame(data[sample(n),])
colnames(shuffled_df) <- gsub("[^a-zA-Z]*", "", colnames(shuffled_df))
c <- colnames(shuffled_df)
train <- shuffled_df[1: round(0.7*n),]
validation <- shuffled_df[(round(0.7*n)+1):round(0.9*n),]
test <- shuffled_df[(round(0.9*n)+1):n,]
X_train <- train[,head(colnames(shuffled_df), -3)] %>% as.matrix()
y_train <- train[,tail(colnames(shuffled_df), 3)] %>% as.matrix()
X_validation <- validation[,head(colnames(shuffled_df), -3)] %>% as.matrix()
y_validation <- validation[,tail(colnames(shuffled_df), 3)] %>% as.matrix()
X_test <- test[,head(colnames(shuffled_df), -3)] %>% as.matrix()
y_test <- test[,tail(colnames(shuffled_df), 3)] %>% as.matrix()
I <- length(colnames(X_train))
O <- length(colnames(y_train))
population <- generation(50)
for (individual in 1:nrow(population)) {
population[individual,] = evaluation(population[individual,], 0)
}
library("dummies")
library("gramEvol")
library("jsonlite")
library("keras")
library("neuralnet")
library("stringr")
#install_keras(tensorflow = "gpu")
# Grammar definition
GRAMMAR <- list(
S = gsrule("<a><h>/<z>"),
a = grule("nnnn"), # Update a with many n as value of I.
z = grule("nnn"),  # Update z with many n as value of O.
h = gsrule("<h><h>", "<h><h>", "/<n>"),
n = gsrule("n<n>", "n")
)
I <- 4
id <- 0
O <- 3
generation <- function(n) {
population <- GrammarRandomExpression(CreateGrammar(GRAMMAR), n)
i <- 1
for (individual in population) {
population[[i]] <- list(id = id, architecture = gsub("\"", "", toString(individual)), evaluated = FALSE,
loss = NULL, metric = NULL, saved_model = NULL) # Cleaning up the expression.
i <- i + 1
id <<- id + 1
}
return(as.data.frame(do.call(rbind, population)))
}
evaluation <- function(individual, mode) {
# Neurons extraction
hidden_layers <- numeric(0)
i <- 0
for (layer in head(strsplit(individual$architecture[[1]], "/")[[1]], -1)) {
if (i != 0) {
hidden_layers[i] <- nchar(layer)
}
i <- i + 1
}
# Individual evaluation
model <- keras_model_sequential()
model %>% layer_dense(units = hidden_layers[1], input_shape = c(I), activation = "relu")
for (layer in tail(hidden_layers, 1)) {
model %>% layer_dense(units = layer, activation = "relu")
}
if (mode == 0) {
model %>% layer_dense(units = O, activation = "softmax")
model %>% compile(
optimizer = "adam",
loss = "categorical_crossentropy",
metrics = c("accuracy")
)
} else {
# TODO: Regression NN output layer.
}
history <- model %>% fit(rbind(X_train, X_validation), rbind(y_train, y_validation), validation_split = 0.235294, epochs = 1000, verbose = 0, callbacks = list(
callback_early_stopping(monitor = "val_loss", min_delta = 0, patience = 100, verbose = 1, mode = "auto")
))
png(filename = paste0("data/history/", individual$id, ".png"))
plot(history)
dev.off()
save_model_hdf5(model, paste0("data/model/", individual$architecture, ".h5"))
score <- model %>% evaluate(X_validation, y_validation)
individual$evaluated <- TRUE
individual$loss <- score['loss'][[1]]
individual$metric <- score['acc'][[1]]
print(str_replace_all(individual$architecture, "/", "_"))
individual$saved_model <- paste0("data/model/", str_replace_all(individual$architecture, "/", "_"), "_", individual$id, ".h5")
return(individual)
}
# Tournament selection
selection <- function(no_childs) {
no_parents <- no_childs
matting_pool <- NULL
for (n in c(1:(no_childs/2))) {
parents_sample <- population[sample(nrow(population), no_parents),]
ordered_parents_sample <- parents_sample[order]
matting_pool <- rbind(matting_pool, head(parents_sample[order(parents_sample$loss)], 2))
}
return(matting_pool)
}
crossover <- function(i1, i2) {
i1_non_terms <- str_count("/", i1$architecture)
}
# MAIN ALGORITHM
data <- read.csv("../datasets/classification/iris.csv", header = T, sep = ",")
n <- nrow(data)
aux <- dummy.data.frame(data, names = c("class"), sep = "")
rm("data")
data <- aux
rm("aux")
shuffled_df <- as.data.frame(data[sample(n),])
colnames(shuffled_df) <- gsub("[^a-zA-Z]*", "", colnames(shuffled_df))
c <- colnames(shuffled_df)
train <- shuffled_df[1: round(0.7*n),]
validation <- shuffled_df[(round(0.7*n)+1):round(0.9*n),]
test <- shuffled_df[(round(0.9*n)+1):n,]
X_train <- train[,head(colnames(shuffled_df), -3)] %>% as.matrix()
y_train <- train[,tail(colnames(shuffled_df), 3)] %>% as.matrix()
X_validation <- validation[,head(colnames(shuffled_df), -3)] %>% as.matrix()
y_validation <- validation[,tail(colnames(shuffled_df), 3)] %>% as.matrix()
X_test <- test[,head(colnames(shuffled_df), -3)] %>% as.matrix()
y_test <- test[,tail(colnames(shuffled_df), 3)] %>% as.matrix()
I <- length(colnames(X_train))
O <- length(colnames(y_train))
population <- generation(50)
for (individual in 1:nrow(population)) {
population[individual,] = evaluation(population[individual,], 0)
}
