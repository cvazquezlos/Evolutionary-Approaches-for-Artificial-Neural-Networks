i <- 0
hidden_l <- numeric(0) # Contains the number of hidden layers and the number of neurons of each hidden layer.
for(j in head(layers, -1)) {
if (i!=0) {hidden_l[i] <- nchar(j)}
i <- i+1
}
return(hidden_l)
}
evaluation <- function(word) {
hidden_layers <- extract_neurons(word)
print(hidden_layers)
for (layer in hidden_layers) {
print(layer)
}
model <- keras_model_sequential()
model %>% layer_dense(units = hidden_layers[1], input_shape = c(I), activation = 'relu')
for (layer in tail(hidden_layers, 1)) {
model %>% layer_dense(units = layer, activation = 'relu')
}
if (classification_type == -1) {
model %>% layer_dense(units = O, activation = 'linear')
model %>% compile(
optimizer = 'adam',
loss = 'mse',
metrics = c('accuracy')
)
} else {
model %>% layer_dense(units = O, activation = 'softmax')
model %>% compile(
optimizer = 'adam',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
}
history <- model %>% fit(X_train, y_train, epochs = epochs)
score <- model %>% evaluate(X_validation, y_validation, batch_size = batch_size)
print(score)
}
monitor <- function(results){
cat("--------------------\n")
print(results)
}
data_cleaning("../datasets/classification/iris.csv", ",")
evaluation('nn/nnnn/n/n')
epochs <- 10
batch_size <- 4
data <- NULL
X_train <- NULL      # 70%
y_train <- NULL
X_validation <- NULL # 20%
y_validation <- NULL
X_test <- NULL       # 10%
y_test <- NULL
I <- NULL
O <- NULL
classification_type <- 1 # -1: Regression
# 0: Single-label classification
# 1: Multi-label classification
data_cleaning <- function(url, sep) {
data <<- read.csv(url, header=T, sep=sep)
colnames(data) <- gsub("[^a-zA-Z]*", "", colnames(data))
n <- nrow(data)
if (str_detect(url, "regression")) {
classification_type <<- -1
max <- apply(data, 2, max)
min <- apply(data, 2, min)
scaled_data <- scale(data, center=min, scale=max-min)
shuffled_df <- as.data.frame(scaled_data[sample(n),])
} else {
if (classification_type == 1) {
aux <- dummy.data.frame(data, names=c("class"), sep="")
rm("data")
data <<- aux
}
shuffled_df <- as.data.frame(data[sample(n),])
}
train <- shuffled_df[1: round(0.7*n),]
validation <- shuffled_df[(round(0.7*n)+1):round(0.9*n),]
test <- shuffled_df[(round(0.9*n)+1):n,]
X_train <<- train[,head(colnames(data), -3)] %>% as.matrix()
y_train <<- train[,tail(colnames(data), 3)] %>% as.matrix()
X_validation <<- validation[,head(colnames(data), -3)] %>% as.matrix()
y_validation <<- validation[,tail(colnames(data), 3)] %>% as.matrix()
X_test <<- test[,head(colnames(data), -3)] %>% as.matrix()
y_test <<- test[,tail(colnames(data), 3)] %>% as.matrix()
I <<- length(colnames(X_train))
O <<- length(colnames(y_train))
}
extract_neurons <- function(word) {
layers <- strsplit(toString(word), "/")[[1]]
i <- 0
hidden_l <- numeric(0) # Contains the number of hidden layers and the number of neurons of each hidden layer.
for(j in head(layers, -1)) {
if (i!=0) {hidden_l[i] <- nchar(j)}
i <- i+1
}
return(hidden_l)
}
evaluation <- function(word) {
hidden_layers <- extract_neurons(word)
print(hidden_layers)
for (layer in hidden_layers) {
print(layer)
}
model <- keras_model_sequential()
model %>% layer_dense(units = hidden_layers[1], input_shape = c(I), activation = 'relu')
for (layer in tail(hidden_layers, 1)) {
model %>% layer_dense(units = layer, activation = 'relu')
}
if (classification_type == -1) {
model %>% layer_dense(units = O, activation = 'linear')
model %>% compile(
optimizer = 'adam',
loss = 'mse',
metrics = c('accuracy')
)
} else {
model %>% layer_dense(units = O, activation = 'softmax')
model %>% compile(
optimizer = 'adam',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
}
history <- model %>% fit(X_train, y_train, epochs = epochs)
score <- model %>% evaluate(X_validation, y_validation, batch_size = batch_size)
print(score)
}
monitor <- function(results){
cat("--------------------\n")
print(results)
}
data_cleaning("../datasets/classification/iris.csv", ",")
evaluation('nn/nnnn/n/n')
library("dummies")
library("gramEvol")
library("keras")
library("neuralnet")
library("stringr")
epochs <- 10
batch_size <- 4
data <- NULL
X_train <- NULL      # 70%
y_train <- NULL
X_validation <- NULL # 20%
y_validation <- NULL
X_test <- NULL       # 10%
y_test <- NULL
I <- NULL
O <- NULL
classification_type <- 1 # -1: Regression
# 0: Single-label classification
# 1: Multi-label classification
data_cleaning <- function(url, sep) {
data <<- read.csv(url, header=T, sep=sep)
colnames(data) <- gsub("[^a-zA-Z]*", "", colnames(data))
n <- nrow(data)
if (str_detect(url, "regression")) {
classification_type <<- -1
max <- apply(data, 2, max)
min <- apply(data, 2, min)
scaled_data <- scale(data, center=min, scale=max-min)
shuffled_df <- as.data.frame(scaled_data[sample(n),])
} else {
if (classification_type == 1) {
aux <- dummy.data.frame(data, names=c("class"), sep="")
rm("data")
data <<- aux
}
shuffled_df <- as.data.frame(data[sample(n),])
}
train <- shuffled_df[1: round(0.7*n),]
validation <- shuffled_df[(round(0.7*n)+1):round(0.9*n),]
test <- shuffled_df[(round(0.9*n)+1):n,]
X_train <<- train[,head(colnames(data), -3)] %>% as.matrix()
y_train <<- train[,tail(colnames(data), 3)] %>% as.matrix()
X_validation <<- validation[,head(colnames(data), -3)] %>% as.matrix()
y_validation <<- validation[,tail(colnames(data), 3)] %>% as.matrix()
X_test <<- test[,head(colnames(data), -3)] %>% as.matrix()
y_test <<- test[,tail(colnames(data), 3)] %>% as.matrix()
I <<- length(colnames(X_train))
O <<- length(colnames(y_train))
}
extract_neurons <- function(word) {
layers <- strsplit(toString(word), "/")[[1]]
i <- 0
hidden_l <- numeric(0) # Contains the number of hidden layers and the number of neurons of each hidden layer.
for(j in head(layers, -1)) {
if (i!=0) {hidden_l[i] <- nchar(j)}
i <- i+1
}
return(hidden_l)
}
evaluation <- function(word) {
hidden_layers <- extract_neurons(word)
print(hidden_layers)
for (layer in hidden_layers) {
print(layer)
}
model <- keras_model_sequential()
model %>% layer_dense(units = hidden_layers[1], input_shape = c(I), activation = 'relu')
for (layer in tail(hidden_layers, 1)) {
model %>% layer_dense(units = layer, activation = 'relu')
}
if (classification_type == -1) {
model %>% layer_dense(units = O, activation = 'linear')
model %>% compile(
optimizer = 'adam',
loss = 'mse',
metrics = c('accuracy')
)
} else {
model %>% layer_dense(units = O, activation = 'softmax')
model %>% compile(
optimizer = 'adam',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
}
history <- model %>% fit(X_train, y_train, epochs = epochs)
score <- model %>% evaluate(X_validation, y_validation, batch_size = batch_size)
print(score)
}
monitor <- function(results){
cat("--------------------\n")
print(results)
}
data_cleaning("../datasets/classification/iris.csv", ",")
evaluation('nn/nnnn/n/n')
epochs <- 50
batch_size <- 4
data <- NULL
X_train <- NULL      # 70%
y_train <- NULL
X_validation <- NULL # 20%
y_validation <- NULL
X_test <- NULL       # 10%
y_test <- NULL
I <- NULL
O <- NULL
classification_type <- 1 # -1: Regression
# 0: Single-label classification
# 1: Multi-label classification
data_cleaning <- function(url, sep) {
data <<- read.csv(url, header=T, sep=sep)
colnames(data) <- gsub("[^a-zA-Z]*", "", colnames(data))
n <- nrow(data)
if (str_detect(url, "regression")) {
classification_type <<- -1
max <- apply(data, 2, max)
min <- apply(data, 2, min)
scaled_data <- scale(data, center=min, scale=max-min)
shuffled_df <- as.data.frame(scaled_data[sample(n),])
} else {
if (classification_type == 1) {
aux <- dummy.data.frame(data, names=c("class"), sep="")
rm("data")
data <<- aux
}
shuffled_df <- as.data.frame(data[sample(n),])
}
train <- shuffled_df[1: round(0.7*n),]
validation <- shuffled_df[(round(0.7*n)+1):round(0.9*n),]
test <- shuffled_df[(round(0.9*n)+1):n,]
X_train <<- train[,head(colnames(data), -3)] %>% as.matrix()
y_train <<- train[,tail(colnames(data), 3)] %>% as.matrix()
X_validation <<- validation[,head(colnames(data), -3)] %>% as.matrix()
y_validation <<- validation[,tail(colnames(data), 3)] %>% as.matrix()
X_test <<- test[,head(colnames(data), -3)] %>% as.matrix()
y_test <<- test[,tail(colnames(data), 3)] %>% as.matrix()
I <<- length(colnames(X_train))
O <<- length(colnames(y_train))
}
extract_neurons <- function(word) {
layers <- strsplit(toString(word), "/")[[1]]
i <- 0
hidden_l <- numeric(0) # Contains the number of hidden layers and the number of neurons of each hidden layer.
for(j in head(layers, -1)) {
if (i!=0) {hidden_l[i] <- nchar(j)}
i <- i+1
}
return(hidden_l)
}
evaluation <- function(word) {
hidden_layers <- extract_neurons(word)
print(hidden_layers)
for (layer in hidden_layers) {
print(layer)
}
model <- keras_model_sequential()
model %>% layer_dense(units = hidden_layers[1], input_shape = c(I), activation = 'relu')
for (layer in tail(hidden_layers, 1)) {
model %>% layer_dense(units = layer, activation = 'relu')
}
if (classification_type == -1) {
model %>% layer_dense(units = O, activation = 'linear')
model %>% compile(
optimizer = 'adam',
loss = 'mse',
metrics = c('accuracy')
)
} else {
model %>% layer_dense(units = O, activation = 'softmax')
model %>% compile(
optimizer = 'adam',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
}
history <- model %>% fit(X_train, y_train, epochs = epochs)
score <- model %>% evaluate(X_validation, y_validation, batch_size = batch_size)
print(score)
}
monitor <- function(results){
cat("--------------------\n")
print(results)
}
data_cleaning("../datasets/classification/iris.csv", ",")
evaluation('nn/nnnn/n/n')
evaluation('nn/nnnn/n')
evaluation <- function(word) {
hidden_layers <- extract_neurons(word)
print(hidden_layers)
for (layer in hidden_layers) {
print(layer)
}
model <- keras_model_sequential()
model %>% layer_dense(units = hidden_layers[1], input_shape = c(I), activation = 'relu')
for (layer in tail(hidden_layers, 1)) {
model %>% layer_dense(units = layer, activation = 'relu')
}
if (classification_type == -1) {
model %>% layer_dense(units = O, activation = 'linear')
model %>% compile(
optimizer = 'adam',
loss = 'mse',
metrics = c('accuracy')
)
} else {
model %>% layer_dense(units = O, activation = 'softmax')
model %>% compile(
optimizer = 'adam',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
}
history <- model %>% fit(X_train, y_train, epochs = epochs)
score <- model %>% evaluate(X_validation, y_validation, batch_size = batch_size)
print(score['loss'])
}
evaluation('nn/nnnn/n')
loss <- NULL
evaluation <- function(word) {
hidden_layers <- extract_neurons(word)
print(hidden_layers)
for (layer in hidden_layers) {
print(layer)
}
model <- keras_model_sequential()
model %>% layer_dense(units = hidden_layers[1], input_shape = c(I), activation = 'relu')
for (layer in tail(hidden_layers, 1)) {
model %>% layer_dense(units = layer, activation = 'relu')
}
if (classification_type == -1) {
model %>% layer_dense(units = O, activation = 'linear')
model %>% compile(
optimizer = 'adam',
loss = 'mse',
metrics = c('accuracy')
)
} else {
model %>% layer_dense(units = O, activation = 'softmax')
model %>% compile(
optimizer = 'adam',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
}
history <- model %>% fit(X_train, y_train, epochs = epochs)
score <- model %>% evaluate(X_validation, y_validation, batch_size = batch_size)
loss <<- score['loss']
}
evaluation('nn/nnnn/n')
View(loss)
loss[1]
View(loss)
loss['loss']
loss[[1]]
loss <- NULL
evaluation <- function(word) {
hidden_layers <- extract_neurons(word)
print(hidden_layers)
for (layer in hidden_layers) {
print(layer)
}
model <- keras_model_sequential()
model %>% layer_dense(units = hidden_layers[1], input_shape = c(I), activation = 'relu')
for (layer in tail(hidden_layers, 1)) {
model %>% layer_dense(units = layer, activation = 'relu')
}
if (classification_type == -1) {
model %>% layer_dense(units = O, activation = 'linear')
model %>% compile(
optimizer = 'adam',
loss = 'mse',
metrics = c('accuracy')
)
} else {
model %>% layer_dense(units = O, activation = 'softmax')
model %>% compile(
optimizer = 'adam',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
}
history <- model %>% fit(X_train, y_train, epochs = epochs)
score <- model %>% evaluate(X_validation, y_validation, batch_size = batch_size)
loss <<- score['loss'][[1]]
}
evaluation('nn/nnnn/n')
epochs <- 50
batch_size <- 4
data <- NULL
X_train <- NULL      # 70%
y_train <- NULL
X_validation <- NULL # 20%
y_validation <- NULL
X_test <- NULL       # 10%
y_test <- NULL
I <- NULL
O <- NULL
classification_type <- 1 # -1: Regression
# 0: Single-label classification
# 1: Multi-label classification
data_cleaning <- function(url, sep) {
data <<- read.csv(url, header=T, sep=sep)
colnames(data) <- gsub("[^a-zA-Z]*", "", colnames(data))
n <- nrow(data)
if (str_detect(url, "regression")) {
classification_type <<- -1
max <- apply(data, 2, max)
min <- apply(data, 2, min)
scaled_data <- scale(data, center=min, scale=max-min)
shuffled_df <- as.data.frame(scaled_data[sample(n),])
} else {
if (classification_type == 1) {
aux <- dummy.data.frame(data, names=c("class"), sep="")
rm("data")
data <<- aux
}
shuffled_df <- as.data.frame(data[sample(n),])
}
train <- shuffled_df[1: round(0.7*n),]
validation <- shuffled_df[(round(0.7*n)+1):round(0.9*n),]
test <- shuffled_df[(round(0.9*n)+1):n,]
X_train <<- train[,head(colnames(data), -3)] %>% as.matrix()
y_train <<- train[,tail(colnames(data), 3)] %>% as.matrix()
X_validation <<- validation[,head(colnames(data), -3)] %>% as.matrix()
y_validation <<- validation[,tail(colnames(data), 3)] %>% as.matrix()
X_test <<- test[,head(colnames(data), -3)] %>% as.matrix()
y_test <<- test[,tail(colnames(data), 3)] %>% as.matrix()
I <<- length(colnames(X_train))
O <<- length(colnames(y_train))
}
extract_neurons <- function(word) {
layers <- strsplit(toString(word), "/")[[1]]
i <- 0
hidden_l <- numeric(0) # Contains the number of hidden layers and the number of neurons of each hidden layer.
for(j in head(layers, -1)) {
if (i!=0) {hidden_l[i] <- nchar(j)}
i <- i+1
}
return(hidden_l)
}
evaluation <- function(word) {
hidden_layers <- extract_neurons(word)
print(hidden_layers)
for (layer in hidden_layers) {
print(layer)
}
model <- keras_model_sequential()
model %>% layer_dense(units = hidden_layers[1], input_shape = c(I), activation = 'relu')
for (layer in tail(hidden_layers, 1)) {
model %>% layer_dense(units = layer, activation = 'relu')
}
if (classification_type == -1) {
model %>% layer_dense(units = O, activation = 'linear')
model %>% compile(
optimizer = 'adam',
loss = 'mse',
metrics = c('accuracy')
)
} else {
model %>% layer_dense(units = O, activation = 'softmax')
model %>% compile(
optimizer = 'adam',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
}
history <- model %>% fit(X_train, y_train, epochs = epochs)
score <- model %>% evaluate(X_validation, y_validation, batch_size = batch_size)
return(score['loss'][[1]])
}
monitor <- function(results){
cat("--------------------\n")
print(results)
}
data_cleaning("../datasets/classification/iris.csv", ",")
evaluation('nn/nnnn/n')
evaluation('nn/nnnnn/n')
evaluation('nn/nnnnn/n')
