if (i!=0) {hidden_l[i] <- nchar(j)}
i <- i+1
}
return(hidden_l)
}
evaluation <- function(word) {
hidden_layers <- extract_neurons(word)
print(hidden_layers)
for (layer in hidden_layers) {
print(layer)
}
model <- keras_model_sequential()
model %>% layer_input(shape = I)
for (layer in hidden_layers) {
model %>% layer_dense(units = layer, activation = 'relu')
}
summary(model)
}
data_cleaning("../datasets/classification/iris.csv", ",")
install_keras()
reticulate
install.packages("reticulate")
install.packages("reticulate")
library("reticulate")
library("reticulate")
library("dummies")
library("gramEvol")
library("keras")
library("neuralnet")
library("stringr")
install_keras()
install.packages("keras")
install.packages("tensorlow")
install.packages("tensorflow")
install_keras()
install_tensorflow()
library("dummies")
library("gramEvol")
library("keras")
library("neuralnet")
library("stringr")
install_keras()
data <- NULL
X_train <- NULL      # 70%
y_train <- NULL
X_validation <- NULL # 20%
y_validation <- NULL
X_test <- NULL       # 10%
y_test <- NULL
I <- NULL
O <- NULL
classification_type <- 1 # 0: Single-label classification
# 1: Multi-label classification
data_cleaning <- function(url, sep) {
data <<- read.csv(url, header=T, sep=sep)
colnames(data) <- gsub("[^a-zA-Z]*", "", colnames(data))
n <- nrow(data)
if (str_detect(url, "regression")) {
max <- apply(data, 2, max)
min <- apply(data, 2, min)
scaled_data <- scale(data, center=min, scale=max-min)
shuffled_df <- as.data.frame(scaled_data[sample(n),])
} else {
if (classification_type == 1) {
aux <- dummy.data.frame(data, names=c("class"), sep="")
rm("data")
data <<- aux
}
shuffled_df <- as.data.frame(data[sample(n),])
}
train <- shuffled_df[1: round(0.7*n),]
validation <- shuffled_df[(round(0.7*n)+1):round(0.9*n),]
test <- shuffled_df[(round(0.9*n)+1):n,]
X_train <<- train[,head(colnames(data), -3)]
y_train <<- train[,tail(colnames(data), 3)]
X_validation <<- validation[,head(colnames(data), -3)]
y_validation <<- validation[,tail(colnames(data), 3)]
X_test <<- test[,head(colnames(data), -3)]
y_test <<- test[,tail(colnames(data), 3)]
I <<- length(colnames(X_train))
O <<- length(colnames(y_train))
}
extract_neurons <- function(word) {
layers <- strsplit(toString(word), "/")[[1]]
i <- 0
hidden_l <- numeric(0) # Contains the number of hidden layers and the number of neurons of each hidden layer.
for(j in head(layers, -1)) {
if (i!=0) {hidden_l[i] <- nchar(j)}
i <- i+1
}
return(hidden_l)
}
evaluation <- function(word) {
hidden_layers <- extract_neurons(word)
print(hidden_layers)
for (layer in hidden_layers) {
print(layer)
}
model <- keras_model_sequential()
model %>% layer_input(shape = I)
for (layer in hidden_layers) {
model %>% layer_dense(units = layer, activation = 'relu')
}
summary(model)
}
data_cleaning("../datasets/classification/iris.csv", ",")
evaluation("nn/nn/n/nnn")
i = [1,2,3]
i <- [1,2,3]
I <- [12,3]
I <- c(2)
I
I[0]
I[1]
library("dummies")
library("gramEvol")
library("keras")
library("neuralnet")
library("stringr")
data <- NULL
X_train <- NULL      # 70%
y_train <- NULL
X_validation <- NULL # 20%
y_validation <- NULL
X_test <- NULL       # 10%
y_test <- NULL
I <- NULL
O <- NULL
classification_type <- 1 # 0: Single-label classification
# 1: Multi-label classification
data_cleaning <- function(url, sep) {
data <<- read.csv(url, header=T, sep=sep)
colnames(data) <- gsub("[^a-zA-Z]*", "", colnames(data))
n <- nrow(data)
if (str_detect(url, "regression")) {
max <- apply(data, 2, max)
min <- apply(data, 2, min)
scaled_data <- scale(data, center=min, scale=max-min)
shuffled_df <- as.data.frame(scaled_data[sample(n),])
} else {
if (classification_type == 1) {
aux <- dummy.data.frame(data, names=c("class"), sep="")
rm("data")
data <<- aux
}
shuffled_df <- as.data.frame(data[sample(n),])
}
train <- shuffled_df[1: round(0.7*n),]
validation <- shuffled_df[(round(0.7*n)+1):round(0.9*n),]
test <- shuffled_df[(round(0.9*n)+1):n,]
X_train <<- train[,head(colnames(data), -3)]
y_train <<- train[,tail(colnames(data), 3)]
X_validation <<- validation[,head(colnames(data), -3)]
y_validation <<- validation[,tail(colnames(data), 3)]
X_test <<- test[,head(colnames(data), -3)]
y_test <<- test[,tail(colnames(data), 3)]
I <<- length(colnames(X_train))
O <<- length(colnames(y_train))
}
extract_neurons <- function(word) {
layers <- strsplit(toString(word), "/")[[1]]
i <- 0
hidden_l <- numeric(0) # Contains the number of hidden layers and the number of neurons of each hidden layer.
for(j in head(layers, -1)) {
if (i!=0) {hidden_l[i] <- nchar(j)}
i <- i+1
}
return(hidden_l)
}
evaluation <- function(word) {
hidden_layers <- extract_neurons(word)
print(hidden_layers)
for (layer in hidden_layers) {
print(layer)
}
model <- keras_model_sequential()
model %>% layer_dense(units = hidden_layers[1], input_shape = c(I), activation = 'relu')
for (layer in tail(hidden_layers, 1)) {
model %>% layer_dense(units = layer, activation = 'relu')
}
summary(model)
}
data_cleaning("../datasets/classification/iris.csv", ",")
evaluation("nn/nn/n/n")
evaluation <- function(word) {
hidden_layers <- extract_neurons(word)
print(hidden_layers)
for (layer in hidden_layers) {
print(layer)
}
model <- keras_model_sequential()
model %>% layer_dense(units = hidden_layers[1], input_shape = c(I), activation = 'relu')
for (layer in tail(hidden_layers, 1)) {
model %>% layer_dense(units = layer, activation = 'relu')
}
if (classification_type == -1) {
model %>% layer_dense(units = O, activation = 'linear')
model %>% compile(
optimizer = 'adam',
loss = 'mean_squared_error',
metrics = c('accuracy')
)
} else {
model %>% layer_dense(units = O, activation = 'sotmax')
model %>% compile(
optimizer = 'adam',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
}
summary(model)
}
evaluation("nn/nn/n/n")
evaluation <- function(word) {
hidden_layers <- extract_neurons(word)
print(hidden_layers)
for (layer in hidden_layers) {
print(layer)
}
model <- keras_model_sequential()
model %>% layer_dense(units = hidden_layers[1], input_shape = c(I), activation = 'relu')
for (layer in tail(hidden_layers, 1)) {
model %>% layer_dense(units = layer, activation = 'relu')
}
if (classification_type == -1) {
model %>% layer_dense(units = O, activation = 'linear')
model %>% compile(
optimizer = 'adam',
loss = 'mean_squared_error',
metrics = c('accuracy')
)
} else {
model %>% layer_dense(units = O, activation = 'softmax')
model %>% compile(
optimizer = 'adam',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
}
summary(model)
}
evaluation("nn/nn/n/n")
epochs <- 10
batch_size <- 4
data <- NULL
X_train <- NULL      # 70%
y_train <- NULL
X_validation <- NULL # 20%
y_validation <- NULL
X_test <- NULL       # 10%
y_test <- NULL
I <- NULL
O <- NULL
classification_type <- 1 # -1: Regression
# 0: Single-label classification
# 1: Multi-label classification
data_cleaning <- function(url, sep) {
data <<- read.csv(url, header=T, sep=sep)
colnames(data) <- gsub("[^a-zA-Z]*", "", colnames(data))
n <- nrow(data)
if (str_detect(url, "regression")) {
classification_type <<- -1
max <- apply(data, 2, max)
min <- apply(data, 2, min)
scaled_data <- scale(data, center=min, scale=max-min)
shuffled_df <- as.data.frame(scaled_data[sample(n),])
} else {
if (classification_type == 1) {
aux <- dummy.data.frame(data, names=c("class"), sep="")
rm("data")
data <<- aux
}
shuffled_df <- as.data.frame(data[sample(n),])
}
train <- shuffled_df[1: round(0.7*n),]
validation <- shuffled_df[(round(0.7*n)+1):round(0.9*n),]
test <- shuffled_df[(round(0.9*n)+1):n,]
X_train <<- train[,head(colnames(data), -3)]
y_train <<- train[,tail(colnames(data), 3)]
X_validation <<- validation[,head(colnames(data), -3)]
y_validation <<- validation[,tail(colnames(data), 3)]
X_test <<- test[,head(colnames(data), -3)]
y_test <<- test[,tail(colnames(data), 3)]
I <<- length(colnames(X_train))
O <<- length(colnames(y_train))
}
extract_neurons <- function(word) {
layers <- strsplit(toString(word), "/")[[1]]
i <- 0
hidden_l <- numeric(0) # Contains the number of hidden layers and the number of neurons of each hidden layer.
for(j in head(layers, -1)) {
if (i!=0) {hidden_l[i] <- nchar(j)}
i <- i+1
}
return(hidden_l)
}
evaluation <- function(word) {
hidden_layers <- extract_neurons(word)
print(hidden_layers)
for (layer in hidden_layers) {
print(layer)
}
model <- keras_model_sequential()
model %>% layer_dense(units = hidden_layers[1], input_shape = c(I), activation = 'relu')
for (layer in tail(hidden_layers, 1)) {
model %>% layer_dense(units = layer, activation = 'relu')
}
if (classification_type == -1) {
model %>% layer_dense(units = O, activation = 'linear')
model %>% compile(
optimizer = 'adam',
loss = 'mse',
metrics = c('accuracy')
)
} else {
model %>% layer_dense(units = O, activation = 'softmax')
model %>% compile(
optimizer = 'adam',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
}
history <- model %>% fit(X_train, y_train, epochs = epochs)
score <- model %>% evaluate(X_validation, y_validation, batch_size = batch_size)
print(score)
}
monitor <- function(results){
cat("--------------------\n")
print(results)
}
data_cleaning("../datasets/classification/iris.csv", ",")
evaluation("nn/nnnn/nn/n")
View(X_test)
epochs <- 10
batch_size <- 4
data <- NULL
X_train <- NULL      # 70%
y_train <- NULL
X_validation <- NULL # 20%
y_validation <- NULL
X_test <- NULL       # 10%
y_test <- NULL
I <- NULL
O <- NULL
classification_type <- 1 # -1: Regression
# 0: Single-label classification
# 1: Multi-label classification
data_cleaning <- function(url, sep) {
data <<- read.csv(url, header=T, sep=sep)
colnames(data) <- gsub("[^a-zA-Z]*", "", colnames(data))
n <- nrow(data)
if (str_detect(url, "regression")) {
classification_type <<- -1
max <- apply(data, 2, max)
min <- apply(data, 2, min)
scaled_data <- scale(data, center=min, scale=max-min)
shuffled_df <- as.data.frame(scaled_data[sample(n),])
} else {
if (classification_type == 1) {
aux <- dummy.data.frame(data, names=c("class"), sep="")
rm("data")
data <<- aux
}
shuffled_df <- as.data.frame(data[sample(n),])
}
train <- shuffled_df[1: round(0.7*n),]
validation <- shuffled_df[(round(0.7*n)+1):round(0.9*n),]
test <- shuffled_df[(round(0.9*n)+1):n,]
X_train <<- train[,head(colnames(data), -3)] %>% as.matrix()
y_train <<- train[,tail(colnames(data), 3)]
X_validation <<- validation[,head(colnames(data), -3)] %>% as.matrix()
y_validation <<- validation[,tail(colnames(data), 3)]
X_test <<- test[,head(colnames(data), -3)] %>% as.matrix()
y_test <<- test[,tail(colnames(data), 3)]
I <<- length(colnames(X_train))
O <<- length(colnames(y_train))
}
extract_neurons <- function(word) {
layers <- strsplit(toString(word), "/")[[1]]
i <- 0
hidden_l <- numeric(0) # Contains the number of hidden layers and the number of neurons of each hidden layer.
for(j in head(layers, -1)) {
if (i!=0) {hidden_l[i] <- nchar(j)}
i <- i+1
}
return(hidden_l)
}
evaluation <- function(word) {
hidden_layers <- extract_neurons(word)
print(hidden_layers)
for (layer in hidden_layers) {
print(layer)
}
model <- keras_model_sequential()
model %>% layer_dense(units = hidden_layers[1], input_shape = c(I), activation = 'relu')
for (layer in tail(hidden_layers, 1)) {
model %>% layer_dense(units = layer, activation = 'relu')
}
if (classification_type == -1) {
model %>% layer_dense(units = O, activation = 'linear')
model %>% compile(
optimizer = 'adam',
loss = 'mse',
metrics = c('accuracy')
)
} else {
model %>% layer_dense(units = O, activation = 'softmax')
model %>% compile(
optimizer = 'adam',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
}
history <- model %>% fit(X_train, y_train, epochs = epochs)
score <- model %>% evaluate(X_validation, y_validation, batch_size = batch_size)
print(score)
}
monitor <- function(results){
cat("--------------------\n")
print(results)
}
data_cleaning("../datasets/classification/iris.csv", ",")
evaluation('nn/nnnn/n/n')
batch_size <- 4
data <- NULL
X_train <- NULL      # 70%
y_train <- NULL
X_validation <- NULL # 20%
y_validation <- NULL
X_test <- NULL       # 10%
y_test <- NULL
I <- NULL
O <- NULL
classification_type <- 1 # -1: Regression
# 0: Single-label classification
# 1: Multi-label classification
data_cleaning <- function(url, sep) {
data <<- read.csv(url, header=T, sep=sep)
colnames(data) <- gsub("[^a-zA-Z]*", "", colnames(data))
n <- nrow(data)
if (str_detect(url, "regression")) {
classification_type <<- -1
max <- apply(data, 2, max)
min <- apply(data, 2, min)
scaled_data <- scale(data, center=min, scale=max-min)
shuffled_df <- as.data.frame(scaled_data[sample(n),])
} else {
if (classification_type == 1) {
aux <- dummy.data.frame(data, names=c("class"), sep="")
rm("data")
data <<- aux
}
shuffled_df <- as.data.frame(data[sample(n),])
}
train <- shuffled_df[1: round(0.7*n),]
validation <- shuffled_df[(round(0.7*n)+1):round(0.9*n),]
test <- shuffled_df[(round(0.9*n)+1):n,]
X_train <<- train[,head(colnames(data), -3)] %>% as.matrix()
y_train <<- train[,tail(colnames(data), 3)] %>% as.matrix()
X_validation <<- validation[,head(colnames(data), -3)] %>% as.matrix()
y_validation <<- validation[,tail(colnames(data), 3)] %>% as.matrix()
X_test <<- test[,head(colnames(data), -3)] %>% as.matrix()
y_test <<- test[,tail(colnames(data), 3)] %>% as.matrix()
I <<- length(colnames(X_train))
O <<- length(colnames(y_train))
}
extract_neurons <- function(word) {
layers <- strsplit(toString(word), "/")[[1]]
i <- 0
hidden_l <- numeric(0) # Contains the number of hidden layers and the number of neurons of each hidden layer.
for(j in head(layers, -1)) {
if (i!=0) {hidden_l[i] <- nchar(j)}
i <- i+1
}
return(hidden_l)
}
evaluation <- function(word) {
hidden_layers <- extract_neurons(word)
print(hidden_layers)
for (layer in hidden_layers) {
print(layer)
}
model <- keras_model_sequential()
model %>% layer_dense(units = hidden_layers[1], input_shape = c(I), activation = 'relu')
for (layer in tail(hidden_layers, 1)) {
model %>% layer_dense(units = layer, activation = 'relu')
}
if (classification_type == -1) {
model %>% layer_dense(units = O, activation = 'linear')
model %>% compile(
optimizer = 'adam',
loss = 'mse',
metrics = c('accuracy')
)
} else {
model %>% layer_dense(units = O, activation = 'softmax')
model %>% compile(
optimizer = 'adam',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
}
history <- model %>% fit(X_train, y_train, epochs = epochs)
score <- model %>% evaluate(X_validation, y_validation, batch_size = batch_size)
print(score)
}
monitor <- function(results){
cat("--------------------\n")
print(results)
}
data_cleaning("../datasets/classification/iris.csv", ",")
evaluation('nn/nnnn/n/n')
