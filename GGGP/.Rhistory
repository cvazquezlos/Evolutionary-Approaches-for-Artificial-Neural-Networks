layers <- strsplit(toString(word), "/")[[1]]
i <- 0
hidden_l <- numeric(0) # Contains the number of hidden layers and the number of neurons of each hidden layer.
for(j in head(layers, -1)) {
if (i!=0) {hidden_l[i] <- nchar(j)}
i <- i+1
}
return(hidden_l)
}
data_cleaning("../datasets/classification/iris.csv", ",")
View(X_test)
dummy.data.frame(train, names=c("class"))
library("dummies")
library("gramEvol")
library("keras")
library("neuralnet")
library("stringr")
library("tensorflow")
data <- NULL
X_train <- NULL      # 70%
y_train <- NULL
X_validation <- NULL # 20%
y_validation <- NULL
X_test <- NULL       # 10%
y_test <- NULL
input <- NULL
output <- NULL
I <- NULL
O <- NULL
classification_type <- 1 # 0: Single-label classification
# 1: Multi-label classification
data_cleaning <- function(url, sep) {
data <<- read.csv(url, header=T, sep=sep)
colnames(data) <- gsub("[^a-zA-Z]*", "", colnames(data))
n <- nrow(data)
input <<- paste(head(colnames(data), -1), collapse="+")
I <<- length(colnames(data)) - 1
O <<- 1
output <<- paste(tail(colnames(data), 1))
if (str_detect(url, "regression")) {
max <- apply(data, 2, max)
min <- apply(data, 2, min)
scaled_data <- scale(data, center=min, scale=max-min)
shuffled_df <- as.data.frame(scaled_data[sample(n),])
} else {
shuffled_df <- as.data.frame(data[sample(n),])
}
train <- shuffled_df[1: round(0.7*n),]
validation <- shuffled_df[(round(0.7*n)+1):round(0.9*n),]
test <- shuffled_df[(round(0.9*n)+1):n,]
X_train <<- train[,head(colnames(data), -1)]
X_validation <<- validation[,head(colnames(data), -1)]
X_test <<- test[,head(colnames(data), -1)]
if (classification_type == 1) {
y_train <<- dummy.data.frame(train, names=c("class"), sep="")
y_validation <<- dummy.data.frame(validation, names=c("class"), sep="")
y_test <<- dummy.data.frame(test, names=c("class"), sep="")
} else {
y_train <<- train[,tail(colnames(data), 1)]
y_validation <<- validation[,tail(colnames(data), 1)]
y_test <<- test[,tail(colnames(data), 1)]
}
}
extract_neurons <- function(word) {
layers <- strsplit(toString(word), "/")[[1]]
i <- 0
hidden_l <- numeric(0) # Contains the number of hidden layers and the number of neurons of each hidden layer.
for(j in head(layers, -1)) {
if (i!=0) {hidden_l[i] <- nchar(j)}
i <- i+1
}
return(hidden_l)
}
data_cleaning("../datasets/classification/iris.csv", ",")
View(y_test)
head(y_train)
tail(y_train, 1)
library("dummies")
library("gramEvol")
library("keras")
library("neuralnet")
library("stringr")
library("tensorflow")
data <- NULL
X_train <- NULL      # 70%
y_train <- NULL
X_validation <- NULL # 20%
y_validation <- NULL
X_test <- NULL       # 10%
y_test <- NULL
input <- NULL
output <- NULL
I <- NULL
O <- NULL
classification_type <- 1 # 0: Single-label classification
# 1: Multi-label classification
data_cleaning <- function(url, sep) {
data <<- read.csv(url, header=T, sep=sep)
colnames(data) <- gsub("[^a-zA-Z]*", "", colnames(data))
n <- nrow(data)
input <<- paste(head(colnames(data), -1), collapse="+")
I <<- length(colnames(data)) - 1
O <<- 1
output <<- paste(tail(colnames(data), 1))
if (str_detect(url, "regression")) {
max <- apply(data, 2, max)
min <- apply(data, 2, min)
scaled_data <- scale(data, center=min, scale=max-min)
shuffled_df <- as.data.frame(scaled_data[sample(n),])
} else {
if (classification_type == 1) {
aux <- dummy.data.frame(data, names=c("class"), sep="")
data <<- aux[, tail(colnames(aux), 3)]
}
shuffled_df <- as.data.frame(data[sample(n),])
}
train <- shuffled_df[1: round(0.7*n),]
validation <- shuffled_df[(round(0.7*n)+1):round(0.9*n),]
test <- shuffled_df[(round(0.9*n)+1):n,]
X_train <<- train[,head(colnames(data), -1)]
X_validation <<- validation[,head(colnames(data), -1)]
X_test <<- test[,head(colnames(data), -1)]
y_train <<- train[,tail(colnames(data), 1)]
y_validation <<- validation[,tail(colnames(data), 1)]
y_test <<- test[,tail(colnames(data), 1)]
}
extract_neurons <- function(word) {
layers <- strsplit(toString(word), "/")[[1]]
i <- 0
hidden_l <- numeric(0) # Contains the number of hidden layers and the number of neurons of each hidden layer.
for(j in head(layers, -1)) {
if (i!=0) {hidden_l[i] <- nchar(j)}
i <- i+1
}
return(hidden_l)
}
data_cleaning("../datasets/classification/iris.csv", ",")
y_test
library("dummies")
library("gramEvol")
library("keras")
library("neuralnet")
library("stringr")
library("tensorflow")
data <- NULL
X_train <- NULL      # 70%
y_train <- NULL
X_validation <- NULL # 20%
y_validation <- NULL
X_test <- NULL       # 10%
y_test <- NULL
input <- NULL
output <- NULL
I <- NULL
O <- NULL
classification_type <- 1 # 0: Single-label classification
# 1: Multi-label classification
data_cleaning <- function(url, sep) {
data <<- read.csv(url, header=T, sep=sep)
colnames(data) <- gsub("[^a-zA-Z]*", "", colnames(data))
n <- nrow(data)
input <<- paste(head(colnames(data), -1), collapse="+")
I <<- length(colnames(data)) - 1
O <<- 1
output <<- paste(tail(colnames(data), 1))
if (str_detect(url, "regression")) {
max <- apply(data, 2, max)
min <- apply(data, 2, min)
scaled_data <- scale(data, center=min, scale=max-min)
shuffled_df <- as.data.frame(scaled_data[sample(n),])
} else {
if (classification_type == 1) {
aux <- dummy.data.frame(data, names=c("class"), sep="")
data <<- aux[, tail(colnames(aux), 3)]
}
shuffled_df <- as.data.frame(data[sample(n),])
print(shuffled_df)
}
train <- shuffled_df[1: round(0.7*n),]
validation <- shuffled_df[(round(0.7*n)+1):round(0.9*n),]
test <- shuffled_df[(round(0.9*n)+1):n,]
X_train <<- train[,head(colnames(data), -1)]
X_validation <<- validation[,head(colnames(data), -1)]
X_test <<- test[,head(colnames(data), -1)]
y_train <<- train[,tail(colnames(data), 1)]
y_validation <<- validation[,tail(colnames(data), 1)]
y_test <<- test[,tail(colnames(data), 1)]
}
extract_neurons <- function(word) {
layers <- strsplit(toString(word), "/")[[1]]
i <- 0
hidden_l <- numeric(0) # Contains the number of hidden layers and the number of neurons of each hidden layer.
for(j in head(layers, -1)) {
if (i!=0) {hidden_l[i] <- nchar(j)}
i <- i+1
}
return(hidden_l)
}
data_cleaning("../datasets/classification/iris.csv", ",")
library("dummies")
library("gramEvol")
library("keras")
library("neuralnet")
library("stringr")
library("tensorflow")
data <- NULL
X_train <- NULL      # 70%
y_train <- NULL
X_validation <- NULL # 20%
y_validation <- NULL
X_test <- NULL       # 10%
y_test <- NULL
input <- NULL
output <- NULL
I <- NULL
O <- NULL
classification_type <- 1 # 0: Single-label classification
# 1: Multi-label classification
data_cleaning <- function(url, sep) {
data <<- read.csv(url, header=T, sep=sep)
colnames(data) <- gsub("[^a-zA-Z]*", "", colnames(data))
n <- nrow(data)
input <<- paste(head(colnames(data), -1), collapse="+")
I <<- length(colnames(data)) - 1
O <<- 1
output <<- paste(tail(colnames(data), 1))
if (str_detect(url, "regression")) {
max <- apply(data, 2, max)
min <- apply(data, 2, min)
scaled_data <- scale(data, center=min, scale=max-min)
shuffled_df <- as.data.frame(scaled_data[sample(n),])
} else {
if (classification_type == 1) {
aux <- dummy.data.frame(data, names=c("class"), sep="")
print(aux)
data <<- aux[, tail(colnames(aux), 3)]
}
shuffled_df <- as.data.frame(data[sample(n),])
}
train <- shuffled_df[1: round(0.7*n),]
validation <- shuffled_df[(round(0.7*n)+1):round(0.9*n),]
test <- shuffled_df[(round(0.9*n)+1):n,]
X_train <<- train[,head(colnames(data), -1)]
X_validation <<- validation[,head(colnames(data), -1)]
X_test <<- test[,head(colnames(data), -1)]
y_train <<- train[,tail(colnames(data), 1)]
y_validation <<- validation[,tail(colnames(data), 1)]
y_test <<- test[,tail(colnames(data), 1)]
}
extract_neurons <- function(word) {
layers <- strsplit(toString(word), "/")[[1]]
i <- 0
hidden_l <- numeric(0) # Contains the number of hidden layers and the number of neurons of each hidden layer.
for(j in head(layers, -1)) {
if (i!=0) {hidden_l[i] <- nchar(j)}
i <- i+1
}
return(hidden_l)
}
data_cleaning("../datasets/classification/iris.csv", ",")
library("dummies")
library("gramEvol")
library("keras")
library("neuralnet")
library("stringr")
library("tensorflow")
data <- NULL
X_train <- NULL      # 70%
y_train <- NULL
X_validation <- NULL # 20%
y_validation <- NULL
X_test <- NULL       # 10%
y_test <- NULL
input <- NULL
output <- NULL
I <- NULL
O <- NULL
classification_type <- 1 # 0: Single-label classification
# 1: Multi-label classification
data_cleaning <- function(url, sep) {
data <<- read.csv(url, header=T, sep=sep)
colnames(data) <- gsub("[^a-zA-Z]*", "", colnames(data))
n <- nrow(data)
input <<- paste(head(colnames(data), -1), collapse="+")
I <<- length(colnames(data)) - 1
O <<- 1
output <<- paste(tail(colnames(data), 1))
if (str_detect(url, "regression")) {
max <- apply(data, 2, max)
min <- apply(data, 2, min)
scaled_data <- scale(data, center=min, scale=max-min)
shuffled_df <- as.data.frame(scaled_data[sample(n),])
} else {
if (classification_type == 1) {
aux <- dummy.data.frame(data, names=c("class"), sep="")
data <<- aux[, tail(colnames(aux), 3)]
print(data)
}
shuffled_df <- as.data.frame(data[sample(n),])
}
train <- shuffled_df[1: round(0.7*n),]
validation <- shuffled_df[(round(0.7*n)+1):round(0.9*n),]
test <- shuffled_df[(round(0.9*n)+1):n,]
X_train <<- train[,head(colnames(data), -1)]
X_validation <<- validation[,head(colnames(data), -1)]
X_test <<- test[,head(colnames(data), -1)]
y_train <<- train[,tail(colnames(data), 1)]
y_validation <<- validation[,tail(colnames(data), 1)]
y_test <<- test[,tail(colnames(data), 1)]
}
extract_neurons <- function(word) {
layers <- strsplit(toString(word), "/")[[1]]
i <- 0
hidden_l <- numeric(0) # Contains the number of hidden layers and the number of neurons of each hidden layer.
for(j in head(layers, -1)) {
if (i!=0) {hidden_l[i] <- nchar(j)}
i <- i+1
}
return(hidden_l)
}
data_cleaning("../datasets/classification/iris.csv", ",")
library("dummies")
library("gramEvol")
library("keras")
library("neuralnet")
library("stringr")
library("tensorflow")
data <- NULL
X_train <- NULL      # 70%
y_train <- NULL
X_validation <- NULL # 20%
y_validation <- NULL
X_test <- NULL       # 10%
y_test <- NULL
input <- NULL
output <- NULL
I <- NULL
O <- NULL
classification_type <- 1 # 0: Single-label classification
# 1: Multi-label classification
data_cleaning <- function(url, sep) {
data <<- read.csv(url, header=T, sep=sep)
colnames(data) <- gsub("[^a-zA-Z]*", "", colnames(data))
n <- nrow(data)
input <<- paste(head(colnames(data), -1), collapse="+")
I <<- length(colnames(data)) - 1
O <<- 1
output <<- paste(tail(colnames(data), 1))
if (str_detect(url, "regression")) {
max <- apply(data, 2, max)
min <- apply(data, 2, min)
scaled_data <- scale(data, center=min, scale=max-min)
shuffled_df <- as.data.frame(scaled_data[sample(n),])
} else {
if (classification_type == 1) {
aux <- dummy.data.frame(data, names=c("class"), sep="")
rm("data")
data <<- aux[, tail(colnames(aux), 3)]
print(data)
}
shuffled_df <- as.data.frame(data[sample(n),])
}
train <- shuffled_df[1: round(0.7*n),]
validation <- shuffled_df[(round(0.7*n)+1):round(0.9*n),]
test <- shuffled_df[(round(0.9*n)+1):n,]
X_train <<- train[,head(colnames(data), -1)]
X_validation <<- validation[,head(colnames(data), -1)]
X_test <<- test[,head(colnames(data), -1)]
y_train <<- train[,tail(colnames(data), 1)]
y_validation <<- validation[,tail(colnames(data), 1)]
y_test <<- test[,tail(colnames(data), 1)]
}
extract_neurons <- function(word) {
layers <- strsplit(toString(word), "/")[[1]]
i <- 0
hidden_l <- numeric(0) # Contains the number of hidden layers and the number of neurons of each hidden layer.
for(j in head(layers, -1)) {
if (i!=0) {hidden_l[i] <- nchar(j)}
i <- i+1
}
return(hidden_l)
}
data_cleaning("../datasets/classification/iris.csv", ",")
library("dummies")
library("gramEvol")
library("keras")
library("neuralnet")
library("stringr")
library("tensorflow")
data <- NULL
X_train <- NULL      # 70%
y_train <- NULL
X_validation <- NULL # 20%
y_validation <- NULL
X_test <- NULL       # 10%
y_test <- NULL
input <- NULL
output <- NULL
I <- NULL
O <- NULL
classification_type <- 1 # 0: Single-label classification
# 1: Multi-label classification
data_cleaning <- function(url, sep) {
data <<- read.csv(url, header=T, sep=sep)
colnames(data) <- gsub("[^a-zA-Z]*", "", colnames(data))
n <- nrow(data)
input <<- paste(head(colnames(data), -1), collapse="+")
I <<- length(colnames(data)) - 1
O <<- 1
output <<- paste(tail(colnames(data), 1))
if (str_detect(url, "regression")) {
max <- apply(data, 2, max)
min <- apply(data, 2, min)
scaled_data <- scale(data, center=min, scale=max-min)
shuffled_df <- as.data.frame(scaled_data[sample(n),])
} else {
if (classification_type == 1) {
aux <- dummy.data.frame(data, names=c("class"), sep="")
rm("data")
data <<- aux
}
shuffled_df <- as.data.frame(data[sample(n),])
}
train <- shuffled_df[1: round(0.7*n),]
validation <- shuffled_df[(round(0.7*n)+1):round(0.9*n),]
test <- shuffled_df[(round(0.9*n)+1):n,]
X_train <<- train[,head(colnames(data), -1)]
y_train <<- train[,tail(colnames(data), 3)]
X_validation <<- validation[,head(colnames(data), -1)]
y_validation <<- validation[,tail(colnames(data), 3)]
X_test <<- test[,head(colnames(data), -1)]
y_test <<- test[,tail(colnames(data), 3)]
}
extract_neurons <- function(word) {
layers <- strsplit(toString(word), "/")[[1]]
i <- 0
hidden_l <- numeric(0) # Contains the number of hidden layers and the number of neurons of each hidden layer.
for(j in head(layers, -1)) {
if (i!=0) {hidden_l[i] <- nchar(j)}
i <- i+1
}
return(hidden_l)
}
data_cleaning("../datasets/classification/iris.csv", ",")
View(y_test)
View(y_train)
View(X_validation)
library("dummies")
library("gramEvol")
library("keras")
library("neuralnet")
library("stringr")
library("tensorflow")
data <- NULL
X_train <- NULL      # 70%
y_train <- NULL
X_validation <- NULL # 20%
y_validation <- NULL
X_test <- NULL       # 10%
y_test <- NULL
input <- NULL
output <- NULL
I <- NULL
O <- NULL
classification_type <- 1 # 0: Single-label classification
# 1: Multi-label classification
data_cleaning <- function(url, sep) {
data <<- read.csv(url, header=T, sep=sep)
colnames(data) <- gsub("[^a-zA-Z]*", "", colnames(data))
n <- nrow(data)
input <<- paste(head(colnames(data), -1), collapse="+")
I <<- length(colnames(data)) - 1
O <<- 1
output <<- paste(tail(colnames(data), 1))
if (str_detect(url, "regression")) {
max <- apply(data, 2, max)
min <- apply(data, 2, min)
scaled_data <- scale(data, center=min, scale=max-min)
shuffled_df <- as.data.frame(scaled_data[sample(n),])
} else {
if (classification_type == 1) {
aux <- dummy.data.frame(data, names=c("class"), sep="")
rm("data")
data <<- aux
}
shuffled_df <- as.data.frame(data[sample(n),])
}
train <- shuffled_df[1: round(0.7*n),]
validation <- shuffled_df[(round(0.7*n)+1):round(0.9*n),]
test <- shuffled_df[(round(0.9*n)+1):n,]
X_train <<- train[,head(colnames(data), -3)]
y_train <<- train[,tail(colnames(data), 3)]
X_validation <<- validation[,head(colnames(data), -3)]
y_validation <<- validation[,tail(colnames(data), 3)]
X_test <<- test[,head(colnames(data), -3)]
y_test <<- test[,tail(colnames(data), 3)]
}
extract_neurons <- function(word) {
layers <- strsplit(toString(word), "/")[[1]]
i <- 0
hidden_l <- numeric(0) # Contains the number of hidden layers and the number of neurons of each hidden layer.
for(j in head(layers, -1)) {
if (i!=0) {hidden_l[i] <- nchar(j)}
i <- i+1
}
return(hidden_l)
}
data_cleaning("../datasets/classification/iris.csv", ",")
View(X_validation)
View(y_validation)
View(X_test)
View(y_test)
