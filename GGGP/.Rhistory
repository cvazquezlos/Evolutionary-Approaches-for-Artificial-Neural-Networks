avg_test_accuracy <- list()
sol_nn_structure <- list()
data_cleaning <- function(url, sep) {
data <<- read.csv(url, header=T, sep=sep)
n <- nrow(data)
if (str_detect(url, "regression")) {
classification_type <<- -1
max <- apply(data, 2, max)
min <- apply(data, 2, min)
scaled_data <- scale(data, center=min, scale=max-min)
shuffled_df <- as.data.frame(scaled_data[sample(n),])
} else {
if (classification_type == 1) {
aux <- dummy.data.frame(data, names=c("class"), sep="")
rm("data")
data <<- aux
}
shuffled_df <- as.data.frame(data[sample(n),])
}
colnames(shuffled_df) <- gsub("[^a-zA-Z]*", "", colnames(shuffled_df))
train <<- shuffled_df[1: round(0.7*n),]
validation <- shuffled_df[(round(0.7*n)+1):round(0.9*n),]
test <- shuffled_df[(round(0.9*n)+1):n,]
X_train <<- train[,head(colnames(shuffled_df), -3)] %>% as.matrix()
y_train <<- train[,tail(colnames(shuffled_df), 3)] %>% as.matrix()
X_validation <<- validation[,head(colnames(shuffled_df), -3)] %>% as.matrix()
y_validation <<- validation[,tail(colnames(shuffled_df), 3)] %>% as.matrix()
X_test <<- test[,head(colnames(shuffled_df), -3)] %>% as.matrix()
y_test <<- test[,tail(colnames(shuffled_df), 3)] %>% as.matrix()
I <<- length(colnames(X_train))
input <<- paste(colnames(X_train), collapse="+")
O <<- length(colnames(y_train))
output <<- paste(colnames(y_train), collapse="+")
}
extract_neurons <- function(word) {
layers <- strsplit(toString(word), "/")[[1]]
i <- 0
hidden_l <- numeric(0) # Contains the number of hidden layers and the number of neurons of each hidden layer.
for(j in head(layers, -1)) {
if (i!=0) {hidden_l[i] <- nchar(j)}
i <- i+1
}
return(hidden_l)
}
evaluation <- function(word) {
hidden_layers <- extract_neurons(word)
model <- keras_model_sequential()
model %>% layer_dense(units = hidden_layers[1], input_shape = c(I), activation = 'relu')
for (layer in tail(hidden_layers, 1)) {
model %>% layer_dense(units = layer, activation = 'relu')
}
if (classification_type == -1) {
model %>% layer_dense(units = O, activation = 'linear')
model %>% compile(
optimizer = 'adam',
loss = 'mse',
metrics = c('accuracy')
)
} else {
model %>% layer_dense(units = O, activation = 'softmax')
model %>% compile(
optimizer = 'adam',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
}
# https://keras.rstudio.com/articles/training_callbacks.html - STOP TRAINING.
history <- model %>% fit(X_train, y_train, epochs = epochs, verbose = 0, callbacks = list(
callback_csv_logger('./results.csv', separator = ",", append = FALSE)
))
if (mode == 0) {
score <- model %>% evaluate(X_validation, y_validation)
} else {
score <- model %>% evaluate(X_test, y_test)
}
print(score)
print(hidden_layers)
return(score['loss'][[1]])
}
monitor <- function(results){
cat("--------------------\n")
print(results)
}
grammar <- list(
S = gsrule("<a><h>/<z>"),
a = grule(replicate(I, "n")),
z = grule(replicate(O, "n")),
h = gsrule("<h><h>", "/<n>"),
n = gsrule("n<n>", "n")
)
grammarDef <- CreateGrammar(grammar)
data_cleaning("../datasets/classification/iris.csv", ",")
evaluation("nn/nnnn/n")
paste('iris_model', j)
j <- 1
paste('iris_model', j)
paste('iris_model', j, collapse = '_')
paste0('iris_model', j)
paste0('iris_model_', j)
paste0('iris_model_', j, '.h5')
epochs <- 100
data <- NULL
I <- NULL
j <- 1
input <- NULL
mode <- 0
O <- NULL
output <- NULL
train <- NULL
X_train <- NULL      # 70%
y_train <- NULL
X_validation <- NULL # 20%
y_validation <- NULL
X_test <- NULL       # 10%
y_test <- NULL
classification_type <- 1 # -1: Regression
# 0: Single-label classification
# 1: Multi-label classification
sol_train_accuracy <- list()
avg_train_accuracy <- list()
sol_validation_accuracy <- list()
avg_validation_accuracy <- list()
sol_test_accuracy <- list()
avg_test_accuracy <- list()
sol_nn_structure <- list()
sol_model_name <- list()
data_cleaning <- function(url, sep) {
data <<- read.csv(url, header=T, sep=sep)
n <- nrow(data)
if (str_detect(url, "regression")) {
classification_type <<- -1
max <- apply(data, 2, max)
min <- apply(data, 2, min)
scaled_data <- scale(data, center=min, scale=max-min)
shuffled_df <- as.data.frame(scaled_data[sample(n),])
} else {
if (classification_type == 1) {
aux <- dummy.data.frame(data, names=c("class"), sep="")
rm("data")
data <<- aux
}
shuffled_df <- as.data.frame(data[sample(n),])
}
colnames(shuffled_df) <- gsub("[^a-zA-Z]*", "", colnames(shuffled_df))
train <<- shuffled_df[1: round(0.7*n),]
validation <- shuffled_df[(round(0.7*n)+1):round(0.9*n),]
test <- shuffled_df[(round(0.9*n)+1):n,]
X_train <<- train[,head(colnames(shuffled_df), -3)] %>% as.matrix()
y_train <<- train[,tail(colnames(shuffled_df), 3)] %>% as.matrix()
X_validation <<- validation[,head(colnames(shuffled_df), -3)] %>% as.matrix()
y_validation <<- validation[,tail(colnames(shuffled_df), 3)] %>% as.matrix()
X_test <<- test[,head(colnames(shuffled_df), -3)] %>% as.matrix()
y_test <<- test[,tail(colnames(shuffled_df), 3)] %>% as.matrix()
I <<- length(colnames(X_train))
input <<- paste(colnames(X_train), collapse="+")
O <<- length(colnames(y_train))
output <<- paste(colnames(y_train), collapse="+")
}
extract_neurons <- function(word) {
layers <- strsplit(toString(word), "/")[[1]]
i <- 0
hidden_l <- numeric(0) # Contains the number of hidden layers and the number of neurons of each hidden layer.
for(j in head(layers, -1)) {
if (i!=0) {hidden_l[i] <- nchar(j)}
i <- i+1
}
return(hidden_l)
}
evaluation <- function(word) {
hidden_layers <- extract_neurons(word)
model <- keras_model_sequential()
model %>% layer_dense(units = hidden_layers[1], input_shape = c(I), activation = 'relu')
for (layer in tail(hidden_layers, 1)) {
model %>% layer_dense(units = layer, activation = 'relu')
}
if (classification_type == -1) {
model %>% layer_dense(units = O, activation = 'linear')
model %>% compile(
optimizer = 'adam',
loss = 'mse',
metrics = c('accuracy')
)
} else {
model %>% layer_dense(units = O, activation = 'softmax')
model %>% compile(
optimizer = 'adam',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
}
# https://keras.rstudio.com/articles/training_callbacks.html - STOP TRAINING.
history <- model %>% fit(X_train, y_train, epochs = epochs, verbose = 0, callbacks = list(history))
print(history)
if (mode == 0) {
score <- model %>% evaluate(X_validation, y_validation)
return(score['loss'][[1]])
} else {
score <- model %>% evaluate(X_test, y_test)
save_model_hdf5(model, paste0('iris_model_', j, '.h5'))
return(score['loss'][[1]])
}
}
monitor <- function(results){
cat("--------------------\n")
print(results)
}
grammar <- list(
S = gsrule("<a><h>/<z>"),
a = grule(replicate(I, "n")),
z = grule(replicate(O, "n")),
h = gsrule("<h><h>", "/<n>"),
n = gsrule("n<n>", "n")
)
grammarDef <- CreateGrammar(grammar)
data_cleaning("../datasets/classification/iris.csv", ",")
evaluation("nn/nnnn/n")
epochs <- 100
data <- NULL
I <- NULL
j <- 1
input <- NULL
mode <- 0
O <- NULL
output <- NULL
train <- NULL
X_train <- NULL      # 70%
y_train <- NULL
X_validation <- NULL # 20%
y_validation <- NULL
X_test <- NULL       # 10%
y_test <- NULL
classification_type <- 1 # -1: Regression
# 0: Single-label classification
# 1: Multi-label classification
sol_train_accuracy <- list()
avg_train_accuracy <- list()
sol_validation_accuracy <- list()
avg_validation_accuracy <- list()
sol_test_accuracy <- list()
avg_test_accuracy <- list()
sol_nn_structure <- list()
sol_model_name <- list()
data_cleaning <- function(url, sep) {
data <<- read.csv(url, header=T, sep=sep)
n <- nrow(data)
if (str_detect(url, "regression")) {
classification_type <<- -1
max <- apply(data, 2, max)
min <- apply(data, 2, min)
scaled_data <- scale(data, center=min, scale=max-min)
shuffled_df <- as.data.frame(scaled_data[sample(n),])
} else {
if (classification_type == 1) {
aux <- dummy.data.frame(data, names=c("class"), sep="")
rm("data")
data <<- aux
}
shuffled_df <- as.data.frame(data[sample(n),])
}
colnames(shuffled_df) <- gsub("[^a-zA-Z]*", "", colnames(shuffled_df))
train <<- shuffled_df[1: round(0.7*n),]
validation <- shuffled_df[(round(0.7*n)+1):round(0.9*n),]
test <- shuffled_df[(round(0.9*n)+1):n,]
X_train <<- train[,head(colnames(shuffled_df), -3)] %>% as.matrix()
y_train <<- train[,tail(colnames(shuffled_df), 3)] %>% as.matrix()
X_validation <<- validation[,head(colnames(shuffled_df), -3)] %>% as.matrix()
y_validation <<- validation[,tail(colnames(shuffled_df), 3)] %>% as.matrix()
X_test <<- test[,head(colnames(shuffled_df), -3)] %>% as.matrix()
y_test <<- test[,tail(colnames(shuffled_df), 3)] %>% as.matrix()
I <<- length(colnames(X_train))
input <<- paste(colnames(X_train), collapse="+")
O <<- length(colnames(y_train))
output <<- paste(colnames(y_train), collapse="+")
}
extract_neurons <- function(word) {
layers <- strsplit(toString(word), "/")[[1]]
i <- 0
hidden_l <- numeric(0) # Contains the number of hidden layers and the number of neurons of each hidden layer.
for(j in head(layers, -1)) {
if (i!=0) {hidden_l[i] <- nchar(j)}
i <- i+1
}
return(hidden_l)
}
evaluation <- function(word) {
hidden_layers <- extract_neurons(word)
model <- keras_model_sequential()
model %>% layer_dense(units = hidden_layers[1], input_shape = c(I), activation = 'relu')
for (layer in tail(hidden_layers, 1)) {
model %>% layer_dense(units = layer, activation = 'relu')
}
if (classification_type == -1) {
model %>% layer_dense(units = O, activation = 'linear')
model %>% compile(
optimizer = 'adam',
loss = 'mse',
metrics = c('accuracy')
)
} else {
model %>% layer_dense(units = O, activation = 'softmax')
model %>% compile(
optimizer = 'adam',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
}
# https://keras.rstudio.com/articles/training_callbacks.html - STOP TRAINING.
history <- model %>% fit(X_train, y_train, epochs = epochs, verbose = 0)
history_df <- as.data.frame(history)
print(history_df)
if (mode == 0) {
score <- model %>% evaluate(X_validation, y_validation)
return(score['loss'][[1]])
} else {
score <- model %>% evaluate(X_test, y_test)
save_model_hdf5(model, paste0('iris_model_', j, '.h5'))
return(score['loss'][[1]])
}
}
monitor <- function(results){
cat("--------------------\n")
print(results)
}
grammar <- list(
S = gsrule("<a><h>/<z>"),
a = grule(replicate(I, "n")),
z = grule(replicate(O, "n")),
h = gsrule("<h><h>", "/<n>"),
n = gsrule("n<n>", "n")
)
grammarDef <- CreateGrammar(grammar)
data_cleaning("../datasets/classification/iris.csv", ",")
evaluation("nn/nnnn/n")
install.packages("rjson")
library("rjson")
evaluation <- function(word) {
hidden_layers <- extract_neurons(word)
model <- keras_model_sequential()
model %>% layer_dense(units = hidden_layers[1], input_shape = c(I), activation = 'relu')
for (layer in tail(hidden_layers, 1)) {
model %>% layer_dense(units = layer, activation = 'relu')
}
if (classification_type == -1) {
model %>% layer_dense(units = O, activation = 'linear')
model %>% compile(
optimizer = 'adam',
loss = 'mse',
metrics = c('accuracy')
)
} else {
model %>% layer_dense(units = O, activation = 'softmax')
model %>% compile(
optimizer = 'adam',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
}
# https://keras.rstudio.com/articles/training_callbacks.html - STOP TRAINING.
history <- model %>% fit(X_train, y_train, epochs = epochs, verbose = 0)
history_df <- as.data.frame(history)
history_json <- toJSON(history_df)
print(history_df)
if (mode == 0) {
score <- model %>% evaluate(X_validation, y_validation)
return(score['loss'][[1]])
} else {
score <- model %>% evaluate(X_test, y_test)
save_model_hdf5(model, paste0('iris_model_', j, '.h5'))
return(score['loss'][[1]])
}
}
evaluation("nn/nnnn/n")
evaluation <- function(word) {
hidden_layers <- extract_neurons(word)
model <- keras_model_sequential()
model %>% layer_dense(units = hidden_layers[1], input_shape = c(I), activation = 'relu')
for (layer in tail(hidden_layers, 1)) {
model %>% layer_dense(units = layer, activation = 'relu')
}
if (classification_type == -1) {
model %>% layer_dense(units = O, activation = 'linear')
model %>% compile(
optimizer = 'adam',
loss = 'mse',
metrics = c('accuracy')
)
} else {
model %>% layer_dense(units = O, activation = 'softmax')
model %>% compile(
optimizer = 'adam',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
}
# https://keras.rstudio.com/articles/training_callbacks.html - STOP TRAINING.
history <- model %>% fit(X_train, y_train, epochs = epochs, verbose = 0)
history_df <- as.data.frame(history)
history_json <- toJSON(history_df)
print(history_json)
if (mode == 0) {
score <- model %>% evaluate(X_validation, y_validation)
return(score['loss'][[1]])
} else {
score <- model %>% evaluate(X_test, y_test)
save_model_hdf5(model, paste0('iris_model_', j, '.h5'))
return(score['loss'][[1]])
}
}
evaluation("nn/nnnn/n")
View(data)
data['spal_length']
data[['spal_length']]
data[['sepal_length']]
data['sepal_length']
evaluation <- function(word) {
hidden_layers <- extract_neurons(word)
model <- keras_model_sequential()
model %>% layer_dense(units = hidden_layers[1], input_shape = c(I), activation = 'relu')
for (layer in tail(hidden_layers, 1)) {
model %>% layer_dense(units = layer, activation = 'relu')
}
if (classification_type == -1) {
model %>% layer_dense(units = O, activation = 'linear')
model %>% compile(
optimizer = 'adam',
loss = 'mse',
metrics = c('accuracy')
)
} else {
model %>% layer_dense(units = O, activation = 'softmax')
model %>% compile(
optimizer = 'adam',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
}
# https://keras.rstudio.com/articles/training_callbacks.html - STOP TRAINING.
history <- model %>% fit(X_train, y_train, epochs = epochs, verbose = 0)
history_df <- as.data.frame(history)
print(history_df)
if (mode == 0) {
score <- model %>% evaluate(X_validation, y_validation)
return(score['loss'][[1]])
} else {
score <- model %>% evaluate(X_test, y_test)
save_model_hdf5(model, paste0('iris_model_', j, '.h5'))
return(score['loss'][[1]])
}
}
evaluation("nn/nnnn/n")
history_df <- NULL
evaluation <- function(word) {
hidden_layers <- extract_neurons(word)
model <- keras_model_sequential()
model %>% layer_dense(units = hidden_layers[1], input_shape = c(I), activation = 'relu')
for (layer in tail(hidden_layers, 1)) {
model %>% layer_dense(units = layer, activation = 'relu')
}
if (classification_type == -1) {
model %>% layer_dense(units = O, activation = 'linear')
model %>% compile(
optimizer = 'adam',
loss = 'mse',
metrics = c('accuracy')
)
} else {
model %>% layer_dense(units = O, activation = 'softmax')
model %>% compile(
optimizer = 'adam',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
}
# https://keras.rstudio.com/articles/training_callbacks.html - STOP TRAINING.
history <- model %>% fit(X_train, y_train, epochs = epochs, verbose = 0)
history_df <<- as.data.frame(history)
if (mode == 0) {
score <- model %>% evaluate(X_validation, y_validation)
return(score['loss'][[1]])
} else {
score <- model %>% evaluate(X_test, y_test)
save_model_hdf5(model, paste0('iris_model_', j, '.h5'))
return(score['loss'][[1]])
}
}
evaluation("nn/nnnn/n")
history_df
history$metrics
history['metrics']
history_df$metrics
history_df$metric
history$metric['loss']
history_df$metric['loss']
history_df$metric$loss
history_df$metric[['loss']]
history_df$metric['loss']
levels(history_df$metric)[1]
levels(history_df$metric)(1)
levels(history_df$metric)
history_df$value
install.packages("jsonlite")
install.packages("jsonlite")
library("jsonlite")
install.packages("jsonlite")
library("jsonlite")
toJSON(history_df)
str(toJSON(history_df))
toJSON(history_df)
hula <- c(1:10)
hula
append(hula, c(11:20))
hula <- append(hula, c(11:20))
